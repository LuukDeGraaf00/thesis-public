
# Literature

Accelerate
- Publications
    - [Accelerating Haskell Array Codes with Multicore GPUs](Literature/acc-cuda-damp2011.pdf) (proposal)
        - 4.4.0: get and set for SoA (CUDA).
        - 5.2.1: data transfer (never mutate data: array fusion form of mutating data?)
        - 5.3.1: threads & influence on memory
        - 6.1.0: data transfer overhead
    - [Optimizing Purely Functional GPU Programs](Literature/acc-optim-icfp2013.pdf) (array fusion & sharing)
        - 4.3.0: use elements multiple times / caching
        - 4.4.0: register pressure (live variables) 
    - [Embedding Foreign Code](Literature/acc-ffi-padl2014.pdf)
        - 1.0.0: importance constraining data access patterns
        - 3.3.0: detect read only variables
    - [Optimizing Pure Functional GPU Programs Extended (PhD Thesis)](Literature/TrevorMcDonell_PhD_Thesis.pdf)
        - 2.1.0: data-level parallelism 
        - 2.2.0: cpu vs gpu (memory access latency)
        - 2.4.0: stencil no shared memory reasoning
        - 3.1.3: SoA only if embedded (overloaded)
        - 3.1.6: surface type to representation type (flatten tuples)
        - 3.1.7: encoding properties / index elements
        - 4.1.2: get function
        - 4.2.3: SoA avoid unused elements
        - 4.2.5: eliminate access does not work with intermediate values (?)
        - 4.2.8: use of GPU L2 cache
        - 4.5.2: simulations
        - 4.5.3: reuse memory and explicit de-allocation
        - 4.5.4: fixed chunk size / garbage collector
        - 4.5.5: DMA and page locking
        - 4.6.3: GPU vs CPU in terms of memory
        - 5.1.0: conditionals (in context of sharing)
        - 5.4.2: array indices as affine functions of other indices
        - 6.1.2: data transfer profiling
        - 6.4.2: loop recovery profiling (remove conditional)
        - 6.4.5: explanation to avoid not caching kernels
        - 6.5.2: shared memory in CUDA
    - [Converting Data-Parallelism to Tast-Parallelism by Rewrites](Literature/acc-multidev-fhpc2015.pdf) (multiple devices)
    - [Functional Array Streams](Literature/acc-seq-fhpc2015.pdf) (main memory)
        - 3.3.0: slicing arrays
        - 7.0.0: overlap data transfer
    - [Type-safe Runtime Code Generation: Accelerate to LLVM](Literature/acc-llvm-haskell2015.pdf) (embedding and backends)
        - 7.1.0: maintain memory coalescing
    - [Streaming Irregular Arrays](Literature/acc-seq2-haskell2017.pdf)
        - 2.1.0: cannot slice for irregular arrays
        - 2.2.0: memory chunks
        - 4.2.0: avoid vectorization
    - [Embedded Pattern Matching](Literature/acc-patmat-2022.pdf)
        - 4.0.0: algebraic data type and generics
        - 4.1.0: product types representation as tuples
        - 4.2.0: sum types with tag
        - 5.3.0: use patterns
        - 6.0.0: lookup table for tags
- Relevant Student Thesis
    - [Accelerating Sum Types](Literature/sum-types.pdf) 
        - 2.3.0: SoA and their use for SIMD
        - 2.4.0: different sum types representations
        - 2.5.0: GADT's
        - 2.6.0: generics
        - 3.3.0: comparison between sum type representations
        - 5.5.0: limitations with the implementation and sum types in general
        - 6.2.0: possible optimizations related to rebuilding
    - [Annotating Deeply Embedded Languages](Literature/annotating-deeply-embedded-languages.pdf) (profiling)
        - 4.2.1: loop unrolling and adjacent optimizations
        - 5.4.1: profiling accelerate, frame profiling now possible with tracy.
        - 5.5.1: limitations of array fusion and solving it with inlining
    - [Investigating the Performance of the Implementations of Embedded Languages in Haskell](Literature/accelerate-performance.pdf) (performance)
        - 2.3.0: additional information on tracy
        - 2.4.0: garbage collector
        - 2.5.1: tree-walking interpreters and pointers
        - 2.6.0: byte-code interpreter in accelerate
        - 2.7.0: compilation in accelerate
        - 5.1.0: profiling in accelerate
        - 6.2.0: garbage collector interference
        - 6.3.0: accelerate memory allocation
        - 6.4.0: influence of memory that is not used by the garbage collector
    - [Improving Cache Performance in Structured GPGPU Workloads via Specialized Thread Schedules](Literature/gpu-stencils.pdf) (stencils and cache performance on GPU)
        - 2.1.2: gpu caches
        - 2.4.0: block algorithms
        - 3.4.2: tiling
        - 4.4.0: vertical reading faster than horizontal
    - [Independently Computed Regions in a Data Parallel Array Language](Literature/independent-regions.pdf) (stencils and regions) 
        - 3.2.0: use for regions
        - 4.0.0: write multiple loops instead of if-then else
        - 4.2.0: regions for fast index (no check)
    - [Optimal Fusion in Data-Parallel Languages](Literature/optimal-fusion.pdf) 
        - 2.1.1: types of fusion distinction
        - 2.2.0: rewrite rules for fusion
        - 3.1.2: indexing functions prevents fusion
    - [Accelerating Nested Data Parallelism: Preserving Regularity](Literature/accelerate-nested-data-parallelism.pdf)
        - 2.1.0: parallel computing explanation
        - 3.4.0: irregular vs regular
        - 4.4.0: shape analysis (size)
        - 8.2.0: other data parallel languages

Cache & Memory
- Paper
    - [Optimistic Evaluation: An Adaptive Evaluation Strategy for Non-Strict Programs](Literature/icfp2003.pdf)
- Other Resources
    - [What Every Programmer Should Know About Memory](Literature/cpumemory.pdf)
    - [CPU cache profiling overview](https://en.wikipedia.org/wiki/Cache_performance_measurement_and_metric)

Patterns
- Sum Types
    - [Futhark Structural Sum Types (as elements)](https://futhark-lang.org/blog/2019-08-21-futhark-0.12.1-released.html)
- Entity Component System
    - [Overwatch Implementation](https://www.youtube.com/watch?v=W3aieHjyNvw)
    - [Intrinsics and Cache in Unity](https://www.youtube.com/watch?v=BpwvXkoFcp8)


